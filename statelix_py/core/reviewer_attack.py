"""
Reviewer Attack Simulator: How Will Your Claims Be Destroyed?

Simulates hostile reviewer questioning to reveal where claims are vulnerable.

"ã“ã®æ–‡ã€å¤–ç”Ÿæ€§ã‚’ä»®å®šã—ã¦ã¾ã™ã‚ˆã­ï¼Ÿ"
"""

from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum

try:
    from .claim_compiler import ClaimIR, CompiledClaim, ClaimNature, ClaimScope
    from .claim_budget import ClaimStrength
except ImportError:
    from statelix_py.core.claim_compiler import ClaimIR, CompiledClaim, ClaimNature, ClaimScope
    from statelix_py.core.claim_budget import ClaimStrength


class ReviewerPersona(Enum):
    """Types of hostile reviewers."""
    CAUSAL_SKEPTIC = "causal_skeptic"
    METHODOLOGY_PEDANT = "methodology_pedant"
    SCOPE_CHALLENGER = "scope_challenger"
    REPLICATION_ADVOCATE = "replication_advocate"
    POLICY_CRITIC = "policy_critic"


@dataclass
class ReviewerAttack:
    """
    A simulated reviewer attack on a claim.
    """
    persona: ReviewerPersona
    attack_text: str
    target_aspect: str  # Which part of the claim is attacked
    severity: float  # 0-1, how damaging this attack is
    requires_assumption: Optional[str] = None
    suggested_defense: Optional[str] = None
    
    @property
    def is_fatal(self) -> bool:
        """Is this attack likely fatal to the claim?"""
        return self.severity > 0.7


@dataclass
class AttackReport:
    """
    Full report of simulated reviewer attacks.
    """
    claim: CompiledClaim
    attacks: List[ReviewerAttack]
    survival_probability: float  # 0-1
    weakest_point: Optional[str] = None
    
    def fatal_attacks(self) -> List[ReviewerAttack]:
        return [a for a in self.attacks if a.is_fatal]
    
    def to_markdown(self) -> str:
        md = f"""# Reviewer Attack Simulation

**Survival Probability:** {self.survival_probability:.0%}
**Weakest Point:** {self.weakest_point or 'Unknown'}

---

## Attacks

"""
        for i, attack in enumerate(self.attacks, 1):
            fatal = "ðŸ’€" if attack.is_fatal else "âš ï¸"
            md += f"### {i}. {attack.persona.value} {fatal}\n\n"
            md += f"> {attack.attack_text}\n\n"
            md += f"- **Target:** {attack.target_aspect}\n"
            md += f"- **Severity:** {attack.severity:.0%}\n"
            if attack.suggested_defense:
                md += f"- **Defense:** {attack.suggested_defense}\n"
            md += "\n"
        
        return md


class ReviewerAttackSimulator:
    """
    Simulates hostile reviewer questioning.
    
    Example:
        >>> sim = ReviewerAttackSimulator()
        >>> report = sim.attack(compiled_claim, ir)
        >>> 
        >>> for attack in report.fatal_attacks():
        ...     print(f"FATAL: {attack.attack_text}")
    """
    
    ATTACKS = {
        ReviewerPersona.CAUSAL_SKEPTIC: [
            ("causal", "ã“ã®å› æžœçš„ä¸»å¼µã‚’æ”¯æŒã™ã‚‹è­˜åˆ¥æˆ¦ç•¥ã¯ä½•ã§ã™ã‹ï¼Ÿ", "causal_claim", 0.8, "exogeneity"),
            ("causal", "é€†å› æžœã®å¯èƒ½æ€§ã‚’æŽ’é™¤ã§ãã¾ã™ã‹ï¼Ÿ", "causal_direction", 0.7, None),
            ("causes", "ã€Œå› æžœã€ã¨ã„ã†è¨€è‘‰ã¯å¼·ã™ãŽã¾ã›ã‚“ã‹ï¼Ÿ", "word_choice", 0.6, None),
            ("effect", "ã“ã‚Œã¯å› æžœåŠ¹æžœã§ã™ã‹ã€ãã‚Œã¨ã‚‚ç›¸é–¢ã§ã™ã‹ï¼Ÿ", "interpretation", 0.75, "independence"),
        ],
        ReviewerPersona.METHODOLOGY_PEDANT: [
            ("linear", "ç·šå½¢æ€§ã®ä»®å®šã¯æº€ãŸã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ", "linearity", 0.5, "linearity"),
            ("normal", "æ®‹å·®ã®æ­£è¦æ€§ã‚’æ¤œå®šã—ã¾ã—ãŸã‹ï¼Ÿ", "normality", 0.4, "normality"),
            ("robust", "é ‘å¥æ€§ãƒã‚§ãƒƒã‚¯ã¯è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ", "robustness", 0.6, None),
            ("standard", "æ¨™æº–èª¤å·®ã¯é©åˆ‡ã«è¨ˆç®—ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ", "inference", 0.5, "homoscedasticity"),
        ],
        ReviewerPersona.SCOPE_CHALLENGER: [
            ("general", "ã“ã®çµæžœã¯ä¸€èˆ¬åŒ–ã§ãã¾ã™ã‹ï¼Ÿ", "generalization", 0.6, None),
            ("population", "å¯¾è±¡æ¯é›†å›£ã¯ä½•ã§ã™ã‹ï¼Ÿ", "scope", 0.5, None),
            ("sample", "ã‚µãƒ³ãƒ—ãƒ«é¸æŠžãƒã‚¤ã‚¢ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ", "selection", 0.7, None),
            ("context", "ä»–ã®æ–‡è„ˆã§ã‚‚æˆã‚Šç«‹ã¡ã¾ã™ã‹ï¼Ÿ", "external_validity", 0.55, None),
        ],
        ReviewerPersona.REPLICATION_ADVOCATE: [
            ("data", "ãƒ‡ãƒ¼ã‚¿ã¨ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¾ã™ã‹ï¼Ÿ", "reproducibility", 0.3, None),
            ("result", "äº‹å‰ç™»éŒ²ã¯ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ", "preregistration", 0.4, None),
            ("significant", "p-hackingã®å¯èƒ½æ€§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ", "multiple_testing", 0.65, None),
        ],
        ReviewerPersona.POLICY_CRITIC: [
            ("policy", "æ”¿ç­–å«æ„ã‚’è¿°ã¹ã‚‹ã«ã¯è¨¼æ‹ ãŒä¸ååˆ†ã§ã¯ï¼Ÿ", "policy_leap", 0.8, None),
            ("should", "ã€Œã¹ãã€ã¨ã„ã†è¡¨ç¾ã¯ç ”ç©¶ã®ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚", "normative", 0.75, None),
            ("recommend", "æŽ¨å¥¨ã‚’è¡Œã†æ ¹æ‹ ã¯ä½•ã§ã™ã‹ï¼Ÿ", "prescription", 0.7, None),
            ("implement", "å®Ÿè£…ã‚’ç¤ºå”†ã™ã‚‹ã®ã¯æ™‚æœŸå°šæ—©ã§ã¯ï¼Ÿ", "intervention", 0.65, None),
        ],
    }
    
    def attack(self, claim: CompiledClaim, ir: ClaimIR) -> AttackReport:
        """
        Simulate attacks on a compiled claim.
        
        Args:
            claim: The compiled claim to attack
            ir: The claim IR
        
        Returns:
            AttackReport with simulated attacks
        """
        attacks = []
        text_lower = claim.text.lower()
        
        for persona, attack_templates in self.ATTACKS.items():
            for trigger, question, target, base_severity, assumption in attack_templates:
                # Check if attack is relevant
                if trigger.lower() in text_lower or self._is_concept_present(ir, trigger):
                    # Adjust severity based on IR
                    severity = self._adjust_severity(base_severity, ir, assumption)
                    
                    defense = self._suggest_defense(ir, assumption, target)
                    
                    attack = ReviewerAttack(
                        persona=persona,
                        attack_text=question,
                        target_aspect=target,
                        severity=severity,
                        requires_assumption=assumption,
                        suggested_defense=defense
                    )
                    attacks.append(attack)
        
        # Calculate survival
        if attacks:
            max_severity = max(a.severity for a in attacks)
            avg_severity = sum(a.severity for a in attacks) / len(attacks)
            survival = 1.0 - (max_severity * 0.6 + avg_severity * 0.4)
        else:
            survival = 0.95
        
        # Find weakest point
        weakest = None
        if attacks:
            worst = max(attacks, key=lambda a: a.severity)
            weakest = worst.target_aspect
        
        return AttackReport(
            claim=claim,
            attacks=attacks,
            survival_probability=max(0, survival),
            weakest_point=weakest
        )
    
    def _is_concept_present(self, ir: ClaimIR, concept: str) -> bool:
        """Check if a concept is implied by the IR."""
        concept_map = {
            'causal': ir.nature == ClaimNature.CAUSAL,
            'policy': ir.nature == ClaimNature.PRESCRIPTIVE,
            'general': ir.scope == ClaimScope.POPULATION_GENERAL,
            'linear': 'linearity' not in ir.assumptions_required,
        }
        return concept_map.get(concept.lower(), False)
    
    def _adjust_severity(
        self, 
        base: float, 
        ir: ClaimIR, 
        assumption: Optional[str]
    ) -> float:
        """Adjust attack severity based on IR strength."""
        severity = base
        
        # Stronger claims are harder to defend
        strength_modifier = {
            ClaimStrength.NONE: -0.3,
            ClaimStrength.MINIMAL: -0.2,
            ClaimStrength.WEAK: -0.1,
            ClaimStrength.MODERATE: 0.0,
            ClaimStrength.STRONG: 0.1,
            ClaimStrength.DEFINITIVE: 0.2,
        }
        severity += strength_modifier.get(ir.strength, 0)
        
        # If assumption is in forbidden list, severity increases
        if assumption and assumption in ir.forbidden_concepts:
            severity += 0.15
        
        return min(1.0, max(0.0, severity))
    
    def _suggest_defense(
        self,
        ir: ClaimIR,
        assumption: Optional[str],
        target: str
    ) -> Optional[str]:
        """Suggest a defense for this attack."""
        if ir.robustness_score > 0.7:
            return "é ‘å¥æ€§åˆ†æžã«ã‚ˆã‚Šã€çµæžœã¯å®‰å®šã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã›ã¾ã™ã€‚"
        
        if target == "scope":
            return "ã‚µãƒ³ãƒ—ãƒ«ã®é™ç•Œã‚’æ˜Žç¤ºçš„ã«èªã‚ã‚‹ã“ã¨ã§å¯¾å¿œã§ãã¾ã™ã€‚"
        
        if target == "causal_claim":
            if ir.nature != ClaimNature.CAUSAL:
                return "å› æžœçš„è§£é‡ˆã‚’é¿ã‘ã€ç›¸é–¢ã¨ã—ã¦ã®ã¿è§£é‡ˆã—ã¦ãã ã•ã„ã€‚"
            return "è­˜åˆ¥æˆ¦ç•¥ã®è©³ç´°ãªèª¬æ˜Žã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
        
        return None


def simulate_reviewer_attacks(
    claim: CompiledClaim,
    ir: ClaimIR
) -> AttackReport:
    """Convenience function to simulate attacks."""
    return ReviewerAttackSimulator().attack(claim, ir)
